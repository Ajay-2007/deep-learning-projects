{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "end_to_end_machine_learning_on_gcp_with_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg7s7cEtugvA",
        "colab_type": "text"
      },
      "source": [
        "Structure of an Estimator API ML model\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "# Define input feature columns\n",
        "\n",
        "featcols = [\n",
        "  tf.feature_column.numeric_column(\"sq_footage\")\n",
        "]\n",
        "\n",
        "# Instantiate Linear Regression Model\n",
        "model = tf.estimator.LinearRegressor(featcols, './model_trained')\n",
        "\n",
        "\n",
        "# Train\n",
        "def train_input_fn():\n",
        "  ...\n",
        "  return features, labels\n",
        "\n",
        "model.train(train_input_fn, steps=100)\n",
        "\n",
        "\n",
        "# Predict\n",
        "def pred_input_fn():\n",
        "  ...\n",
        "  \n",
        "  return features\n",
        "\n",
        "out = model.predict(pred_input_fn\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp6_ArHWvQui",
        "colab_type": "text"
      },
      "source": [
        "# Encoding categorical data to suply to a DNN\n",
        "\n",
        "```python\n",
        "\n",
        "# 1a. If you know the complete vocavulary beforehand:\n",
        "\n",
        "tf.feature_column.categorical_column_with_vocabulary_list('zipcode',\n",
        "volcabulary_list = ['83452', '72345', '87654', '98723', '23451']),\n",
        "\n",
        "# 1b. If your data is already indexed; i.e., has integers in [0-N):\n",
        "\n",
        "tf.feature_column.categorical_column_with_identity('stateId', num_buckets=50)\n",
        "\n",
        "# 2. To pass in a categorical column into a DNN, one option is to one-hot encode it:\n",
        "\n",
        "tf.feature_column.indicator_column(my_categorical_column)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZnf9W-ovQzm",
        "colab_type": "text"
      },
      "source": [
        "# To read CSV files, create a TextLineDataset giving it a function to decode the CSV into features, labels\n",
        "\n",
        "```python\n",
        "CSV_COLUMNS = ['sqfootage', 'city', 'amount']\n",
        "LABEL_COLUMN  = 'amount'\n",
        "DEFAULTS = [[0.0], ['na'], [0.0]]\n",
        "\n",
        "def read_dataset(filename, mode, batch_size=512):\n",
        "  def decode_csv(value_column):\n",
        "    columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
        "    features = dict(zip(CSV_COLUMNS, columns))\n",
        "    label = features.pop(LABEL_COLUMN)\n",
        "    return features, label\n",
        "\n",
        "  dataset = tf.data.TextLineDataset(filename).map(decode_csv)\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    num_epochs = None # indefinitely\n",
        "    dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
        "  else:\n",
        "    num_epochs = 1 # end-of-input after this\n",
        "  \n",
        "  dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
        "  \n",
        "  return dataset.make_one_shot_iterator().get_next()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqP4vyyovQq7",
        "colab_type": "text"
      },
      "source": [
        "# Estimator API comes with a method that handles distributed training and evaluation\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "estimator = tf.estimator.LinearRegressor(\n",
        "  model_dir = output_dir,\n",
        "  features_columns=feature_cols\n",
        ")\n",
        "\n",
        "tf.estimator.train_and_evaluate(\n",
        "  estimator,\n",
        "  train_spec,\n",
        "  eval_spec\n",
        ")\n",
        "```\n",
        "# Benifits of using `tf.estimator.train_and_evaluate`\n",
        "* Distribute the graph\n",
        "* Share variables\n",
        "* Evaluate occasionally\n",
        "* Handle machine failures\n",
        "* Create checkpoint files\n",
        "* Recover from failures\n",
        "* Save summaries for TensorBoard\n",
        "\n",
        "# TrainSpec consists of the things that used to be passed into the train() method\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(\n",
        "  input_fn=read_dataset('gs://.../train*',\n",
        "  mode = tf.contrib.learn.ModeKeys.TRAIN,\n",
        "  max_steps=num_train_steps\n",
        ")\n",
        "\n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "\n",
        "```\n",
        "\n",
        "# Think `steps` not `epochs`, with production-ready, distributed models.\n",
        "\n",
        "1. Gradient updates from slow workers could get ignored\n",
        "2. When retraining a model with fresh data, we'll resume from earlier numbers of steps ( and corresponding hyper-parameters)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZuZsdYaAnTf",
        "colab_type": "text"
      },
      "source": [
        "# EvalSpec controls the evaluation and the checkpointing of the model because they happen at the same time\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "exporter = ...\n",
        "\n",
        "eval_spec = tf.estimator.EvalSpec(\n",
        "  input_fn = read_dataset('gs://.../valid*',\n",
        "                          mode=tf.contrib.learn.ModeKeys.EVAL),\n",
        "  steps=None,\n",
        "  start_delay_secs=60 # start evaluating after N seconds\n",
        "  throttle_secsx=600,\n",
        "  exporters=exporter\n",
        ")\n",
        "\n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVmT1Jl0Ana9",
        "colab_type": "text"
      },
      "source": [
        "# Wide-and-deep network in Estimator API\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.estimator.DNNLinearCombinedClassifier(\n",
        "  model_dir=...,\n",
        "  linear_feature_columns=wide_columns,\n",
        "  dnn_feature_columns=deep_columns,\n",
        "  dnn_hidden_units=[100, 50]\n",
        "  \n",
        ")\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICN-lKETAnXf",
        "colab_type": "text"
      },
      "source": [
        "# Monitor and experiment with training\n",
        "\n",
        "```python\n",
        "from google.datalab.ml import TensorBoard\n",
        "\n",
        "TensorBoard().start('./babyweight_trained')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znV2HWkSZEIN",
        "colab_type": "text"
      },
      "source": [
        "# Operationalize the model\n",
        "\n",
        "```python\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "(p\n",
        "\n",
        "  | beam.io.ReadFromText('gs://..')\n",
        "\n",
        "  | bea.Map(Transform)\n",
        "\n",
        "  | beam.GroupByKey()\n",
        "\n",
        "  | beam.FlatMap(Filter)\n",
        "\n",
        "  | beam.io.WriteToText('gs://...')\n",
        "\n",
        ")\n",
        "\n",
        "p.run()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD50zoE9ZEO7",
        "colab_type": "text"
      },
      "source": [
        "# An example Beam pipeline for BigQuery->CSV on cloud\n",
        "\n",
        "```python\n",
        "\n",
        "import apache_beam as beam\n",
        "\n",
        "def transform(rowdict):\n",
        "  import copy\n",
        "  result = copy.deepcopy(rowdict)\n",
        "  if rowdict['a'] > 0::\n",
        "    result['c'] = result['a'] * result['b']\n",
        "    yeild ','.join([str(result[k]) if k in result else 'None' for k in ['a', 'b', 'c']])\n",
        "\n",
        "  \n",
        "if __name__ == \"__main__\":\n",
        "  p = beam.Pipeline(argv=sys.argv)\n",
        "\n",
        "  selquery = 'SELECT a,b FROM someds.sometable'\n",
        "\n",
        "  (p\n",
        "  \n",
        "    |  beam.io.Read(beam.io.BigQuerySource(query=selquery,\n",
        "                                          user_standard_sql=True))\n",
        "    |  beam.Map(transform_data) # do some processing\n",
        "    | beam.io.WriteToText('gs://...') # write output\n",
        "  )\n",
        "\n",
        "  p.run() # run the pipeline\n",
        "\n",
        "\n",
        "```\n",
        "```text\n",
        "\n",
        "Simply running main() runs pipeline locally\n",
        "\n",
        "\n",
        "python ./etl.py\n",
        "\n",
        "To run on cloud, specify cloud parameters\n",
        "\n",
        "python ./etl.py \\\n",
        "      --project=$PROJECT\\\n",
        "      --job_name=myjob\\\n",
        "      --stagging_location=gs://$BUCKET/staging/\\\n",
        "      --temp_location=gs://$BUCKET/staging/\\\n",
        "      --runner=DataflowRunner # DirectRunner would be local\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PifA9wPZEF0",
        "colab_type": "text"
      },
      "source": [
        "# Use the gcloud command to submit the training job either locally or to the cloud\n",
        "\n",
        "```bash\n",
        "\n",
        "gcloud ml-engine local train \\\n",
        "    --module-name=trainer.task\\\n",
        "    --package-path=/somedir/babyweight/trainer\\\n",
        "    --train_data_paths etc.\n",
        "\n",
        "\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        "    --region=$REGION\\\n",
        "    --module-name=trainer.task\\\n",
        "    --job-dir=$OUTDIR \\\n",
        "    --staging-bucket=gs://$BUCKET \\\n",
        "    --scale-tier=BASIC\\\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOaThFywhIOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym54_04wuc4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v60SGBtKuk8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}