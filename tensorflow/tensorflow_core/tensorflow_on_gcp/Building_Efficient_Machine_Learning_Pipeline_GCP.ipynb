{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building Efficient Machine Learning Pipeline GCP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxEXp7r3Z1AA",
        "colab_type": "text"
      },
      "source": [
        "# Using native TensorFlow ops to read images\n",
        "\n",
        "```python\n",
        "# Read an image from a file, decodes it into a dense tensor, and resize it to a fixed shape.\n",
        "\n",
        "\n",
        "def _parse_function(filename, label):\n",
        "  image_string = tf.read_file(filename)\n",
        "  image_decoded = tf.image.decode_image(image_string)\n",
        "  image_resized = tf.image.resize_images(image_decoded, [299, 299])\n",
        "\n",
        "  return image_resized, label\n",
        "\n",
        "\n",
        "# A vector of filenames\n",
        "file_list = tf.gfile.Glob(filename)\n",
        "filenames = tf.constant(file_list)\n",
        "\n",
        "# labels[i] is the label for the image in filenames[i].\n",
        "\n",
        "labels = tf.constant(label_list)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "dataset = dataset.map(_parse_function)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsm5dUlPZ09T",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess data into TFReocord\n",
        "\n",
        "```python\n",
        "\n",
        "def convert_to_example(csvline, categories):\n",
        "  filename, label = csvline.encode('ascii', 'ignore').split(',')\n",
        "  if label in categories:\n",
        "    coder = ImageCoder()\n",
        "    image_buffer, height, width = _get_image_data(filename, coder)\n",
        "    example = _convert_to_example(filename, image_buffer, categories.index(label), label, height, width)\n",
        "\n",
        "    yeild example.SerializeToString()\n",
        "\n",
        "LABELS = ['nails', 'screws']\n",
        "\n",
        "(p\n",
        "  | beam.FlatMap(lambda line: convert_to_example(line, LABELS))\n",
        "  | beam.io.tfrecordio.WriteToTFRecord(os.path.join(OUTPUT_DIR, 'train'))\n",
        ")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4It-gtuZ06e",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "\n",
        "def input_fn(batch_size):\n",
        "  files = tf.data.Dataset.list_files(file_pattern)\n",
        "  dataset = tf.data.TFRecordDataset(files, num_parallel_reads=40)\n",
        "  dataset = dataset.shuffle(buffer_size=10000)\n",
        "  dataset = dataset.repeat(NUM_EPOCHS)\n",
        "  dataset = dataset.map(preproc_fn, num_parallel_calls=40)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "\n",
        "  # Prefetch pipelines everything above with the accelerator training\n",
        "  dataset = dataset.prefetch(buffer_size=1)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KharHc3SZ03j",
        "colab_type": "text"
      },
      "source": [
        "# Using fused transformation ops\n",
        "\n",
        "```python\n",
        "\n",
        "def input_fn(batch_size):\n",
        "  files = tf.data.Dataset.list_files(file_pattern)\n",
        "  dataset = tf.data.TFRecordDataset(files, num_parallel_reads=40)\n",
        "\n",
        "\n",
        "  dataset = dataset.apply(\n",
        "    tf.contrib.data.shuffle_and_repeat(buffer_size=10000, NUM_EPOCHS)\n",
        "\n",
        "  dataset = dataset.apply(\n",
        "    tf.contrib.data.map_and_batch(parser_fn, batch_size))\n",
        "\n",
        "  dataset = dataset.prefetch(buffer_size=1)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyCCPGqXZ00a",
        "colab_type": "text"
      },
      "source": [
        "# Training with Estimator API\n",
        "\n",
        "```python\n",
        "\n",
        "# MirroredStrategy for multi GPU distribution\n",
        "distribution = tf.contrib.distribute.MirroredStrategy()\n",
        "\n",
        "\n",
        "# Pass the distribution to RunConfig\n",
        "run_config = tf.estimator.RunConfig(train_distribute=distribution)\n",
        "\n",
        "classifier = tf.estimator.Estimator(\n",
        "  model_fn = model_function,\n",
        "  model_dir = model_dir,\n",
        "  config=run_config)\n",
        "\n",
        "classifier.train(input_fn=input_function)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ejmHVNZ0xx",
        "colab_type": "text"
      },
      "source": [
        "# `train_and_evaluate` bundles together a distributed workflow\n",
        "\n",
        "```python\n",
        "\n",
        "def train_and_evaluate(output_dir, config, params):\n",
        "  features = [tf.feature_column.embedding_columns(...),\n",
        "              tf.feature_column.bucketized_column(...)]\n",
        "  \n",
        "  estimator = tf.estimator.Estimator(model_fn = simple_rnn,\n",
        "                                     model_dir = output_dir)\n",
        "\n",
        "  train_spec = tf.estimator.TrainSpec(input_fn = get_train(),\n",
        "                                      max_steps = 1000)\n",
        "\n",
        "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
        "  eval_spec = tf.estimator.EvalSpec(input_fn = get_valid(),\n",
        "                                    steps = None,\n",
        "                                    exporters = exporter)\n",
        "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "\n",
        "\n",
        "# Runs training, evaluation, etc. on Cloud ML\n",
        "train_and_evaluate(output_dir)\n",
        "\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJK-jlgvZw32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "``"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}