{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Collaborative Recommendation Systems_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHwyb26C_d3M",
        "colab_type": "text"
      },
      "source": [
        "# Using Neural Network for Content-Based  Recommendation Systems\n",
        "\n",
        "```python\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "content_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "  key=\"content_id\",\n",
        "  hash_bucket_size=len(content_ids_list))\n",
        "\n",
        "embedded_content_column = tf.feature_column.embedding_column(\n",
        "  categorical_column=content_id_column,\n",
        "  dimension=10)\n",
        "\n",
        "embedded_title_column = hub.text_embedding_column(\n",
        "  key=\"title\",\n",
        "  module_spec=\"https://tfhub.dev/google/nnlm-de-dim50/1\",\n",
        "  trainable=False\n",
        ")\n",
        "\n",
        "\n",
        "author_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "\n",
        "  key=\"author\",\n",
        "  hash_bucket_size=len(authors_list) + 1)\n",
        "\n",
        "embedded_author_column = tf.feature_column.embedding_columm(\n",
        "  categorical_column=author_column,\n",
        "  dimension=3\n",
        ")\n",
        "\n",
        "months_since_epoch_column = tf.feature_column.numeric_column(\n",
        "  key=\"months_since_epoch\"\n",
        ")\n",
        "\n",
        "months_since_epoch_bucketized = tf.feature_column.bucketized_column(\n",
        "  source_column=months_since_epoch_column,\n",
        "  boundaries=months_since_epoch_boundaries)\n",
        "\n",
        "\n",
        "net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
        "\n",
        "for units in params['hidden_units']:\n",
        "  net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
        "\n",
        "# Compute logits (1 per class).\n",
        "\n",
        "logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8KtpnbJ_d0p",
        "colab_type": "text"
      },
      "source": [
        "# WALS Collaborative Filtering Implementation in TensorFlow Estimator\n",
        "\n",
        "```python\n",
        "# Because WALS requires whole rows or columns, the data has to be preprocessed to provide SparseTensors of rows/columns\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "grouped_by_items = mapped_df.groupby('itemId')\n",
        "\n",
        "with tf.python_ip.TFReocrdWriter('data/users_for_item') as ofp:\n",
        "  for item, grouped in grouped_by_items:\n",
        "    example = tf.train.Example(features=tf.train.Features(feature={\n",
        "      'key' : tf.train.Feature(int64_list=tf.train.Int64List(value=[item])),\n",
        "      'indices': tf.train.Feature(int64_list=tf.train.Int64List(value=grouped['userId'].values)),\n",
        "      'values': tf.train.Feature(float_list=tf.train.FloatList(value=grouped['rating'].values))\n",
        "    }))\n",
        "\n",
        "    ofp.write(example.SerializeToString())\n",
        "\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Scn7lrt_dyp",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "# Because WALS requires whole rows or columns, the data has to be preprocessed to provide SparseTensors of rows/columns\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "grouped_by_users = mapped_df.groupby('userId')\n",
        "\n",
        "with tf.python_ip.TFReocrdWriter('data/users_for_item') as ofp:\n",
        "  for user, grouped in grouped_by_users:\n",
        "    example = tf.train.Example(features=tf.train.Features(feature={\n",
        "      'key' : tf.train.Feature(int64_list=tf.train.Int64List(value=[user])),\n",
        "      'indices': tf.train.Feature(int64_list=tf.train.Int64List(value=grouped['itemId'].values)),\n",
        "      'values': tf.train.Feature(float_list=tf.train.FloatList(value=grouped['rating'].values))\n",
        "    }))\n",
        "\n",
        "    ofp.write(example.SerializeToString())\n",
        "```\n",
        "\n",
        "\n",
        "# WALS Matrix Factorization Estimator\n",
        "\n",
        "```python\n",
        "\n",
        "tf.contrib.learn.Experiment(\n",
        "\n",
        "  tf.contrib.factorization.WALSMatrixFactorization(\n",
        "    num_rows=args['nusers'], num_cols=args['nitems'],\n",
        "    embedding_dimension=args['n_embeds'],\n",
        "    model_dir=args['output_dir']\n",
        "  ),\n",
        "\n",
        "  train_input_fn = read_dataset(tf.estimator.ModeKeys.TRAIN, args),\n",
        "  eval_input_fn = read_dataset(tf.estimator.ModeKeys.EVAL, args),\n",
        "  train_steps = train_steps,\n",
        "  eval_steps = 1,\n",
        "  min_eval_frequency = steps_in_epoch,\n",
        "  export_strategies = tf.contrib.learn.utils.saved_model_export_utils.make_export_strategy(serving_input_fn=create_serving_input_fn(args))\n",
        ")\n",
        "\n",
        "```\n",
        "# The input function has to read the files and create sparse tensors for the rows and for the columns\n",
        "\n",
        "```python\n",
        "\n",
        "def parse_tfrecords(filename, vocab_size):\n",
        "  files = tf.gfile.Glob(os.path.join(args['input_path'], filename))\n",
        "  dataset = tf.data.TFRecordDataset(files)\n",
        "  dataset = dataset.map(lambda x: decode_example(x, vocab_size))\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    num_epochs = None # indefinitely\n",
        "  else:\n",
        "    num_epochs = 1 # end-of-input after this\n",
        "\n",
        "\n",
        "  dataset = dataset.repeat(num_epochs)\n",
        "  dataset = dataset.batch(args['batch_size'])\n",
        "  dataset = dataset.map(lambda xZ: remap_keys(x))\n",
        "  return dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "\n",
        "def _input_fn():\n",
        "  features = {\n",
        "    WALSMatrixFactorization.INPUT_ROWS: parse_tfrecords('items_for_user', args['nitems']),\n",
        "    WALSMatrixFactorization.INPUT_COLS: parse_tfrecords('users_for_item', args['nuers']),\n",
        "    WALSMatrixFactorization.PROJECT_ROW: tf.constant(True)\n",
        "  }\n",
        "\n",
        "  return features, None\n",
        "```\n",
        "\n",
        "\n",
        "# Decode the TF Record files and invoke sparse_merge to create the necessary SparseTensor\n",
        "```python\n",
        "\n",
        "def decode_example(protos, vocab_size):\n",
        "  \n",
        "  # Specify the features that were saved in the TF Record file\n",
        "  features = {\n",
        "    'key': tf.FixedLenFeature([1], tf.int64),\n",
        "    'indices': tf.VarLenFeature(dtype=tf.int64),\n",
        "    'values': tf.VarLenFeature(dtype=tf.float32)\n",
        "  }\n",
        "  # Parse from the file\n",
        "  parsed_features = tf.parse_single_example(protos, features)\n",
        "  # Create SparseTensor\n",
        "  values = tf.sparse_merge(parsed_features['indices'], parsed_features['values'], vocab_size=vocab_size)\n",
        "\n",
        "  # Save key (itemId or userId)\n",
        "  # Save key to remap after batching is complete\n",
        "  key = parsed_features['key']\n",
        "\n",
        "  # Splice key into SparseTensor with tf.concat\n",
        "\n",
        "  decoded_sparse_tensor = tf.SparseTensor(indices=tf.concat([values.indices, [key]], axis=0),\n",
        "  values = tf.concat([values.values, [0.0]], axis=0), dense_shape = values.dense_shape)\n",
        "\n",
        "  # Return final SparseTensor\n",
        "  return decoded_sparse_tensor\n",
        "\n",
        "```\n",
        "\n",
        "# Remap keys to SparseTensor to fix re-indexing after batching\n",
        "\n",
        "```python\n",
        "\n",
        "def remap_keys(sparse_tensor):\n",
        "  # Current indices of our SparseTensor that we need to fix\n",
        "  bad_indices = sparse_tensor.indices\n",
        "  # Current values of our SparseTensor that we need to fix\n",
        "  bad_values = sparse_tensor.values\n",
        "\n",
        "  # Group by the batch_indices and get the count for each\n",
        "  size = tf.segmen_sum(data = tf.ones_like(bad_indices[:, 0], dtype= tf.int64), segment_ids = bad_indices[:, 0]) - 1\n",
        "\n",
        "  # The number of batch_indices (this should be batch_size unless it is a partially full batch)\n",
        "\n",
        "  length = tf.shape(size, out_type = tf.int64)[0]\n",
        "\n",
        "  # Finds the cumulative sum which we can use for indexing later\n",
        "  cum = tf.cumsum(size)\n",
        "\n",
        "  # The offsets between each example in the batch due to concatenation of the keys in decode_example\n",
        "  length_range = tf.range(start = 0, limit = length, delta = 1, dtype = tf.int64)\n",
        "\n",
        "  # Indices of the SparseTensor of the rows added by concatenation of keys in decode_example\n",
        "\n",
        "  cum_range = cum + length_range\n",
        "\n",
        "  # The keys that we have extracted back out of our concatenated SparseTensor\n",
        "\n",
        "  gathered_indices = tf.squeeze(tf.gather(bad_indices, cum_range)[:, 1])\n",
        "\n",
        "  # The enumerated row indices of the SparseTensor's indices member\n",
        "  sparse_indices_range = tf.range(tf.shape(bad_indices, out_type=tf.int64)[0], dtype=tf.int64)\n",
        "\n",
        "\n",
        "  # Want to find the rows indices of the SparseTensor that are actual data & not the concatenated rows So we can to find the intersection of the two sets and then take the opposite of that\n",
        "\n",
        "  x = sparse_indices_range\n",
        "  s = cum_range\n",
        "\n",
        "  # Number of multiples we are going to tile x, which is our sparse_indices_range\n",
        "\n",
        "  tile_multiples = tf.concat([tf.ones(tf.shape(tf.shape(x)), dtype=tf.int64), tf.shape(s, out_type=tf.int64)], axis=0)\n",
        "\n",
        "  # Expands x, our sparse_indices_range, into a rank 2 tensor\n",
        "  # Then multiplies the rows by 1 (no copying) and the columns by the number of examples in the batch \n",
        "\n",
        "  x_tile = tf.tile(tf.expand_dims(x, -1_, tile_multiples)\n",
        "\n",
        "  # Essentially a vectorized logical or, that we then negate\n",
        "  x_not_in_s = ~tf.reduce_any(tf.equal(x_tile, s), -1) \n",
        "\n",
        "  # The SparseTensor's indices that are our actual data by using the boolean_mask we just made above Applied to the entire indices member of our SparseTensor\n",
        "\n",
        "  selected_indices = tf.boolean_mask(tensor = bad_indices, mask = x_not_in_s, axis = 0)\n",
        "\n",
        "  # Apply the same boolean_mask to the entire values member of our SparseTensor\n",
        "  # Gets the actual values data\n",
        "  selected_values = tf.boolean_mask(tensor = bad_values, mask = x_not_in_s, axis = 0)\n",
        "\n",
        "  # Need to replace the first column of selected_indices with keys\n",
        "  # So we first need to tile gathered_indices\n",
        "  tiling = tf.tile(input = tf.expand_dims(gathered_indices[0], -1_, multiples = tf.expand_dims(size[0], -1))\n",
        "\n",
        "  # We have to repeatedly apply the tiling to each example in the batch\n",
        "  # Since it is jagged we cannot use tf.map_fn due to the stacking of the TensorArray So we have to create our own custom version\n",
        "\n",
        "  def loop_body(i, tensor_grow):\n",
        "    return i + 1, tf.concat(values, [tensor_grow, \n",
        "              tf.tile(input = tf.expand_dims(gathered_indices[i], -1),\n",
        "              multiples = tf.expand_dims(size[i], -1))], axis=0)\n",
        "\n",
        "  _, result = tf.while_loop(lambda i, tensor_grow: i < length, loop_body, [tf.constant(1, dtype=tf.int64), tiling])\n",
        "\n",
        "\n",
        "  # Concatenated tiled keys with the 2nd column of selected_indices\n",
        "\n",
        "  selected_indices_fixed = tf.concat([tf.expand_dims(result, -1),\n",
        "                                      tf.expand_dims(selected_indices[:,1], -1)], axis=1)\n",
        "\n",
        "  # Combine everything together back into a SparseTensor\n",
        "\n",
        "  remapped_sparse_tensor = tf.SparseTensor(indices = selected_indices_fixed, values = selected_values, dense_shape = sparse_tensor.dense_shape)\n",
        "\n",
        "  return remapped_sparse_tensor\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnc_DLR1_dwh",
        "colab_type": "text"
      },
      "source": [
        "# The train_and_evaluate loop is typical of tf.contrib Estimators\n",
        "\n",
        "```python\n",
        "def train_and_evaluate(args):\n",
        "  train_steps = int(0.5 + (1.0 * args['num_epochs'] * args['nusers']) / args['batch_size']\n",
        "\n",
        "  def experiement_fn(output_dir):\n",
        "    return tf.contrib.learn.Experiement(\n",
        "      tf.contrib.factorization.WALSMatrixFactorization(\n",
        "        num_rows=args['nusers'], num_cols=args['nitems'],\n",
        "        embedding_dimension=args['n_embeds'],\n",
        "        model_dir=args['output_dir']),\n",
        "      train_input_fn = read_dataset(tf.estimator.ModeKeys.TRAIN, args),\n",
        "      eval_input_fn = read_dataset(tf.estimator.ModeKeys.EVAL, args),\n",
        "      train_steps=train_steps,\n",
        "      eval_steps=1,\n",
        "      min_eval_frequency=steps_in_epoch,\n",
        "      export_strategies=tf.contrib.learn.utils.saved_model_export_utils.make_export_strategy(serving_input_fn=create_serving_input_fn(args)))\n",
        "\n",
        "  learn_runner.run(experiment_fn, args['output_dir'])\n",
        "\n",
        "def find_top_k(user_factors, item_factors, k):\n",
        "  all_items = tf.matmul(tf.expand_dims(user_factors, 0), tf.transpose(item_factors))\n",
        "\n",
        "  topk = tf.nn.top_k(all_items, k=k)\n",
        "\n",
        "  return tf.cast(topk.indices, dtype=tf.int64)\n",
        "```\n",
        "\n",
        "# Finding top K items for all users can be done as a batch prediction job\n",
        "\n",
        "```python\n",
        "\n",
        "def batch_predict(args):\n",
        "  with tf.Session() as sess:\n",
        "    estimator = tf.contrib.factorization.WALSMatrixFactorization(\n",
        "                        num_rows=args['nusers'], num_cols=args['nitems'],\n",
        "                        embedding_dimension=args['n_embeds'],\n",
        "                        model_dir=args['output_dir'])\n",
        "\n",
        "    user_factors = tf.convert_to_tensor(estimator.get_row_factors()[0]) # (nusers, nembeds)\n",
        "    item_factors = tf.convert_to_tensor(estimator.get_col_factors()[0]) # (nitems, nembeds)\n",
        "\n",
        "    # for each user, find the top K items\n",
        "\n",
        "    topk = tf.squeeze(tf.map_fn(lambda user: find_top_k(user, item_factors, args['topk']), user_factors, dtype=tf.int64))\n",
        "\n",
        "    with file_io.FileIO(os.path.join(args['output_dir'], 'batch_pred.txt', mode='w') as f:\n",
        "      for best_items_for_user in topk.eval():\n",
        "        f.write(','.join(str(x) for x in best_items_for_user) + '\\n')\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muDrgmsz_dtl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVrLn19A_dq7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJg7LO33_KvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}