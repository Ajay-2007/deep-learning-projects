{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How_to_start_using_a_dataset_part_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5W38gt15lPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdvb-6wG5tn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(tfds.core.GeneratorBasedBuilder):\n",
        "  VERSION = tfds.core.Version('0.1.0')\n",
        "\n",
        "  def _into(self):\n",
        "    return tfds.core.DatasetInfo(builder=self, description=..., features=...,\n",
        "                                 supervised_keys=..., utls=..., citation=...)\n",
        "  \n",
        "  def _split_generators(self, dl_manager):\n",
        "\n",
        "  def _generate_examples(self):\n",
        "    yeild 'key', {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gRwAWBBKf9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tfds.core.FileSp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41iHiPQI6JO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(tfds.core.GeneratorBasedBuilder):\n",
        "  VERSION = tfds.core.Version('0.1.0')\n",
        "\n",
        "  def _into(self):\n",
        "    return tfds.core.DatasetInfo(\n",
        "        builder=self, description=(\"INSERT DESCRIPTION HERE\"),\n",
        "        features=tfds.features.FeatureDict({\n",
        "            \"description\": tfds.features.Text(),\n",
        "            \"image\": tfds.features.Image(),\n",
        "            \"label\": tfds.features.ClassLabel(num_classes=5),\n",
        "        }),\n",
        "\n",
        "        supervised_keys=(\"image\", \"label\")\n",
        "        urls = [\"https://dataset-homepage.org\"],\n",
        "        citation=r\"\"\"@article{my-awesome-dataset-2020,\n",
        "                  ...\n",
        "                  author = {Smith, John}, \"}\"\"\"\n",
        "    )\n",
        "  \n",
        "  def _split_generators(self, dl_manager):\n",
        "\n",
        "  def _generate_examples(self):\n",
        "    yeild 'key', {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNtujpoc7T8K",
        "colab_type": "text"
      },
      "source": [
        "# Example of a real dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PStsZ-u46JMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(tfds.core.GeneratorBasedBuilder):\n",
        "  VERSION = tfds.core.Version('0.1.0')\n",
        "\n",
        "  def _into(self):\n",
        "    return tfds.core.DatasetInfo(\n",
        "        builder=self,\n",
        "        description=\"A large set of images of horses and humans.\",\n",
        "        features=tfds.features.FeatureDict({\n",
        "            \"image\": tfds.features.Image(shape=_IMAGE_SHAPE),\n",
        "            \"label\": tfds.features.ClassLabel(\n",
        "                names=[\"horses\", \"humans\"]\n",
        "            )\n",
        "        }),\n",
        "\n",
        "        supervised_keys=(\"image\", \"label\"),\n",
        "        urls=[\"http://laurencemoroney.com/horses-or-humans-dataset\"],\n",
        "        citation=_CITATION\n",
        "    )\n",
        "  \n",
        "  def _split_generators(self, dl_manager):\n",
        "    \"\"\"Returns SplitGenerators.\"\"\"\n",
        "\n",
        "    # TODO(my_dataset): Downloads the data and defines the splits\n",
        "    # dl_manager is a tfds.download.DownloadManger that can be used to \n",
        "    # download and extract URLs\n",
        "\n",
        "    \n",
        "    # Equivalent to dl_manager.extract(dl_manager.download(urls))\n",
        "\n",
        "    # dl_paths = dl_manager.download_and_extract({\n",
        "    #     'foo': 'https://example.com/foo.zip',\n",
        "    #     'bar': 'https://example.com/bar.zip',\n",
        "    # })\n",
        "\n",
        "    # dl_paths['foo'], dl_paths['bar']\n",
        "\n",
        "    return [\n",
        "            tfds.core.SplitGenerator(\n",
        "                name=tfds.Split.TRAIN,\n",
        "                # These kwargs will be passed to _generate_examples\n",
        "                gen_kwargs={},\n",
        "            ),\n",
        "    ]\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  def _generate_examples(self):\n",
        "    yeild 'key', {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YCrizTR6JIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _split_generators(self, dl_manager):\n",
        "  return [tfds.core.SplitGenerator(\n",
        "      name=tfds.Split.TRAIN,\n",
        "      gen_kwargs={\n",
        "          \"dir_path\": os.path.join(extracted_path, \"train\"),\n",
        "          \"labels\": os.path.join(extracted_path, \"train_labels.csv\")}),\n",
        "          tfds.core.SplitGenerator(\n",
        "              name=tfds.Split.TEST,\n",
        "              gen_kwargs={\n",
        "                  \"dir_path\": os.path.join(extracted_path, \"test\"),\n",
        "                  \"labels\": os.path.join(extracted_path, \"test_labels.csv\")})\n",
        "  ]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXLPneiY6JDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _split_generators(self, dl_manager):\n",
        "  train_path, test_path = dl_manager.download([_TRAIN_URL, _TEST_URL])\n",
        "\n",
        "  return [\n",
        "    tfds.core.SplitGenerator(\n",
        "        name=tfds.Split.TRAIN,\n",
        "        num_shards=10,\n",
        "        gen_kwargs={\n",
        "            \"archive\": dl_manager.iter_archive(train_path)\n",
        "            }),\n",
        "    tfds.core.SplitGenerator(\n",
        "        name=tfds.Split.TEST,\n",
        "        num_shards=10,\n",
        "        gen_kwargs={\n",
        "            \"archive\": dl_manager.iter_archive(test_path)\n",
        "        }),\n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_isB3PK66I5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _generatoe_examples(self):\n",
        "  \"\"\"Yields examples\"\"\"\n",
        "  # TODO(my_dataset): Yields (key, example) tuples from the dataset\n",
        "  yield 'key', {}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofo9663e-2oN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _generate_examples(self, archive):\n",
        "  for fname, fobj in archive:\n",
        "    res = _NAME_RE.match(fname)\n",
        "\n",
        "    if not res: # if anything other than .png; skip\n",
        "      continue\n",
        "    label = res.group(1).lower()\n",
        "\n",
        "    record = {\n",
        "        \"image\": fobj,\n",
        "        \"label\": label,\n",
        "    }\n",
        "\n",
        "    yield fname, record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHUy1z6S-22x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "builder._generate_examples(\n",
        "    images_dir_path=\"{extracted_path}/train\",\n",
        "    labels=\"{extracted_path}/train_labels.csv\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mqNFajK_9L0",
        "colab_type": "text"
      },
      "source": [
        "# File access and possible problem in data\n",
        "\n",
        "* Use `tf.io.gfile` or `tf.python_io` to support Cloud Storage systems\n",
        "* Avoid using Python built-ins\n",
        "  * e.g., `open`, `os.rename`, `gzip`\n",
        "\n",
        "# Extra dependencies\n",
        "* Use `tfds.core.lazy_imports`\n",
        "* Add a lazy import with an entry in DATASET_EXTRAS\n",
        "* Install with `pip install tensorflow-datasets[<dataset-name>]`\n",
        "\n",
        "# Problems in data\n",
        "* Corrupted data\n",
        "  * e.g., invalid image formats\n",
        "* Inconsistent data\n",
        "  * Solution: Mark dataset as unstable by adding A class constant - `UNSTABLE` in `DatasetBuilder` \n",
        "\n",
        "\n",
        "\n",
        "# When to use configurations\n",
        "\n",
        "## `Heavy`\n",
        "* Specifies how data needs to be written to the disk\n",
        "* Different DatasetInfo setups\n",
        "* When changing access for download data\n",
        "* Use `tfds.core.BuilderConfigs` to configure data generation \n",
        "\n",
        "## `Light`\n",
        "* Deals with runtime preprocessing\n",
        "* `tf.data` input pipelines\n",
        "* Perform additional transformations\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAsqsHEE-2ti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12ca7d9d-177c-4cef-d09b-50b11c1e77b0"
      },
      "source": [
        "# Loading a dataset with a custom configuration\n",
        "\n",
        "# See the built-in configs\n",
        "configs = tfds.text.IMDBReviews.builder_configs\n",
        "\n",
        "print(configs.keys())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['plain_text', 'bytes', 'subwords8k', 'subwords32k'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOVvi7Wq-1vH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Address a built-in config with tfds.builder\n",
        "imdb = tfds.builder(\"imdb_reviews/bytes\")\n",
        "# or when constructing the builder directly\n",
        "imdb = tfds.text.IMDBReviews(config=\"bytes\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qrXhI6IDH0s",
        "colab_type": "text"
      },
      "source": [
        "# Publishing the dataset\n",
        "\n",
        "## Add an import for registration\n",
        "\n",
        "```python\n",
        "\n",
        "# In the 'image' subdirectory of tensorflow/datasets\n",
        "\n",
        "from tensorflow_datasets.image.cifar import Cifar10\n",
        "from tensorflow_datasets.image.cifer import Cifer100\n",
        "\n",
        "from tensorflow_datasets.image.my_image_dataset import MyImageDataset\n",
        "\n",
        "# In the 'text' subdirectory of tensorflow/datasets\n",
        "\n",
        "from tensorflow_datasets.text.cnn_dailymail import CnnDailymail\n",
        "\n",
        "\n",
        "from tensorflow_datasets.image.my_text_datset import MyTextDataset\n",
        "```\n",
        "\n",
        "## Download and prepare\n",
        "* Create file\n",
        "\n",
        "```bash\n",
        "tensorflow_datasets/url_checksums/my_new_dataset.txt\n",
        "```\n",
        "\n",
        "* Run `download_and_prepare` locally to ensure that data generation works\n",
        "# default data_dir is ~/tensorflow_datasets\n",
        "\n",
        "```bash\n",
        "python -m tensorflow_datasets.scripts.download_and_prepare \\\n",
        "  --register_checksums \\\n",
        "  --datasets=my_new_dataset\n",
        "\n",
        "```\n",
        "\n",
        "## Test data\n",
        "* Put test data under yout dataset's directory\n",
        "* Make sure there are no duplicates in splits\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow_datasets import my_dataset\n",
        "import tensorflow_datasets.testing as tfds_test\n",
        "\n",
        "class MyDatasetTest(tfds_test.DatasetBuilderTestCase):\n",
        "  DATASET_CLASS = my_dataset.MyDataset\n",
        "  SPLITS = { # Expected number of examples on each split from fake example.\n",
        "  \"train\": 3,\n",
        "  \"test\": 3,\n",
        "  }\n",
        "\n",
        "  # If dataset `download_and_extract`s more than one resource:\n",
        "  DL_EXTRACT_RESULT = {\n",
        "    \"name1\": \"path/to/file1\", # Relative to fake_examples/my_dataset dir.\n",
        "    \"name2\": \"file2\",\n",
        "  } \n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "    tfds_test.test_main()\n",
        "```\n",
        "\n",
        "## Final touches\n",
        "* Make sure coding style is compliant with \n",
        "  * PEP8\n",
        "  * Google's Python Style Guide\n",
        "* Add release notes\n",
        "* Send for review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD80rGPe-1rW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}