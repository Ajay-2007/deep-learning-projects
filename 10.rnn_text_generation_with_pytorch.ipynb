{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10.rnn_text_generation_with_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOgxq6skgbj6A03lkVfBlF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajay-2007/deep-learning-projects/blob/master/10.rnn_text_generation_with_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq3N5flSrND8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgZXn24QsUc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "  def __init__(self):\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.idx = 0\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2idx:\n",
        "      self.word2idx[word] = self.idx\n",
        "      self.idx2word[self.idx] = word\n",
        "      self.idx += 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.word2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESE7yjNzppDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'alice.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBMu5LLPv0Af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextProcess(object):\n",
        "  def __init__(self):\n",
        "    self.dictionary = Dictionary()\n",
        "  \n",
        "  def get_data(self, path, batch_size=20):\n",
        "    with open(path, 'r') as f:\n",
        "      tokens = 0\n",
        "      for line in f:\n",
        "        words = line.split() + ['<eos>']\n",
        "        tokens += len(words)\n",
        "        for word in words:\n",
        "          self.dictionary.add_word(word)\n",
        "    # Create a 1-D tensor that contains the index of all the words in the file\n",
        "    rep_tensor = torch.LongTensor(tokens)\n",
        "    index = 0\n",
        "    with open(path, 'r') as f:\n",
        "      for line in f:\n",
        "        words = line.split() + ['<eos>']\n",
        "        for word in words:\n",
        "          rep_tensor[index] = self.dictionary.word2idx[word]\n",
        "          index += 1\n",
        "    \n",
        "    # Find out how many batches we need\n",
        "    num_batches = rep_tensor.shape[0] // batch_size\n",
        "    # Remove the remainder (Filter out the ones that don't fit)\n",
        "    rep_tensor = rep_tensor[:num_batches*batch_size]\n",
        "    # return (batch_size, num_batches)\n",
        "    rep_tensor = rep_tensor.view(batch_size, -1)\n",
        "    return rep_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bviA3mKZqH4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 128 # Input features to the LSTM\n",
        "hidden_size = 1024 # Number of LSTM units\n",
        "num_layers = 1\n",
        "num_epochs = 20\n",
        "batch_size = 20\n",
        "timesteps = 30  # means we are gonna look at 30 previous words to predict our next word  \n",
        "learning_rate = 0.002\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd_-pOMDq6uQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = TextProcess() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBT2A-P7r-06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rep_tensor = corpus.get_data(path, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJf8aoeZsE2G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8462040b-b271-4a8d-cd4a-0be49b348a82"
      },
      "source": [
        "# rep_tensor is the tensor that contains the index of all the words. Each row contains 1659 words by default\n",
        "# here 1484 words after batchification\n",
        "print(rep_tensor.shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 1484])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8n1rnmusRL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a210e0f-10e0-4a5d-a971-726f3023b3d7"
      },
      "source": [
        "vocab_size = len(corpus.dictionary)\n",
        "print(vocab_size)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "416tz10hs2gM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2845d50-1415-46ba-9ff2-093524886f85"
      },
      "source": [
        "num_batches = rep_tensor.shape[1] // timesteps\n",
        "print(num_batches)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbTXpYF4tD0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e6c48577-7b13-42c1-f6ea-32ad16758ac6"
      },
      "source": [
        "a = torch.rand(5, 7)\n",
        "print(a)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6826, 0.4834, 0.3792, 0.7138, 0.3188, 0.0459, 0.6458],\n",
            "        [0.2459, 0.1989, 0.4913, 0.7054, 0.7653, 0.3471, 0.9858],\n",
            "        [0.8532, 0.4252, 0.0775, 0.3901, 0.9370, 0.6702, 0.3175],\n",
            "        [0.4594, 0.1358, 0.3185, 0.4017, 0.0579, 0.5686, 0.5904],\n",
            "        [0.5284, 0.9979, 0.2478, 0.6939, 0.0532, 0.1556, 0.3775]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv0GXJdKtefW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextGenerator(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "    super(TextGenerator, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, x, h):\n",
        "    # Perform word embedding\n",
        "    x = self.embed(x)\n",
        "    # Reshape the input tensor\n",
        "    # x = x.view(batch_size, timesteps, embed_size)\n",
        "    out, (h, c) = self.lstm(x, h)\n",
        "    # Reshape the output from (samples, timesteps, output_features) to a shape appropriate for the FC layer\n",
        "    # (batch_size*timesteps, hidden_size)\n",
        "    out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "    #Decode hidden states of all the steps\n",
        "    out = self.linear(out)\n",
        "    return out, (h,c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoEYLGMSyBO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TextGenerator(vocab_size, embed_size, hidden_size, num_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kidWtTE2yHBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ5Opht-3F_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CUDA = torch.cuda.is_available()\n",
        "if CUDA:\n",
        "  model = model.cuda()\n",
        "  # optimizer = optimizer.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeL0yhsW3CSY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "f6daa229-56a3-4b8d-c5e2-8af4bd127133"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  # Set initial hidden and cell states\n",
        "  states = (torch.zeros(num_layers, batch_size, hidden_size).cuda(),\n",
        "            torch.zeros(num_layers, batch_size, hidden_size).cuda())\n",
        "  \n",
        "  for i in range(0, rep_tensor.size(1)-timesteps, timesteps):\n",
        "    # Get mini-batch inputs and targets\n",
        "    inputs = rep_tensor[:, i:i+timesteps]   # --> (:, 0:0+30), output-> (:, 1+31)\n",
        "    targets = rep_tensor[:, (i+1):(i+1)+timesteps]\n",
        "    if CUDA:\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "    # String : Black Horse is here\n",
        "    # input: Black Horse    Output: lack Hourse i\n",
        "\n",
        "    outputs, _ = model(inputs, states)\n",
        "    loss = loss_fn(outputs, targets.reshape(-1))\n",
        "\n",
        "    # Backpropagation and Wegith Update\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    # Perform Gradient Clipping. clip_value (float or int) is the maximum allowed value of gradients\n",
        "    # The gradients are clipped in the range [-clip_value, cli_value]. This is to prevent the exploding gradient problem\n",
        "    clip_grad_norm(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    step = (i+1) // timesteps\n",
        "    if step%100 == 0:\n",
        "      print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 8.5782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2/20], Loss: 5.9148\n",
            "Epoch [3/20], Loss: 5.2421\n",
            "Epoch [4/20], Loss: 4.6427\n",
            "Epoch [5/20], Loss: 4.1334\n",
            "Epoch [6/20], Loss: 3.7461\n",
            "Epoch [7/20], Loss: 3.3155\n",
            "Epoch [8/20], Loss: 2.8126\n",
            "Epoch [9/20], Loss: 2.4399\n",
            "Epoch [10/20], Loss: 2.0577\n",
            "Epoch [11/20], Loss: 1.7114\n",
            "Epoch [12/20], Loss: 1.3719\n",
            "Epoch [13/20], Loss: 1.0551\n",
            "Epoch [14/20], Loss: 0.7872\n",
            "Epoch [15/20], Loss: 0.5136\n",
            "Epoch [16/20], Loss: 0.3774\n",
            "Epoch [17/20], Loss: 0.2023\n",
            "Epoch [18/20], Loss: 0.1223\n",
            "Epoch [19/20], Loss: 0.0896\n",
            "Epoch [20/20], Loss: 0.0769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB34SrHu814Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dbfc0ec1-e0ac-4f96-f6df-66fd03516344"
      },
      "source": [
        "# Test the model\n",
        "model = model.cpu()\n",
        "with torch.no_grad():\n",
        "  with open('results.txt', 'w') as f:\n",
        "    # Set initial hidden one cell states\n",
        "    state = (torch.zeros(num_layers, 1, hidden_size),\n",
        "             torch.zeros(num_layers, 1, hidden_size))\n",
        "    # Select one word id randomly and convert it to shape (1, 1)\n",
        "    input = torch.randint(0, vocab_size, (1,)).long().unsqueeze(1)\n",
        "\n",
        "    for i in range(500):\n",
        "      output, _ = model(input, state)\n",
        "      # print(output.shape)\n",
        "      # Sample a word id from the exponential of the output\n",
        "      prob = output.exp() \n",
        "      word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "      # print(word_id)\n",
        "      # print(word_id.shape)\n",
        "\n",
        "      # Replace the input with sampled word id for the next time step\n",
        "      input.fill_(word_id)\n",
        "\n",
        "      # Write the results to file\n",
        "      word = corpus.dictionary.idx2word[word_id]\n",
        "      word = '\\n' if word == '<eos>' else word+' '\n",
        "      f.write(word)\n",
        "\n",
        "      if (i+1)%100 == 0:\n",
        "        print('Sampled [{}/{}] words and save to {}'.format(i+1, 500, 'results.txt'))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sampled [100/500] words and save to results.txt\n",
            "Sampled [200/500] words and save to results.txt\n",
            "Sampled [300/500] words and save to results.txt\n",
            "Sampled [400/500] words and save to results.txt\n",
            "Sampled [500/500] words and save to results.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkZ9bp_zEud7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b294a29-466e-45bd-af15-6e3b7d849c01"
      },
      "source": [
        "with open('results.txt', 'r') as f:\n",
        "  print(f.read())"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "she is you see.' However, the Gryphon. 'They told sweet-tempered. that, and Alice was a small \n",
            "\"What but hurriedly 'Why, now in a bough and \n",
            "it unfolded the silence. \n",
            "\n",
            "'Well, I might venture to the first \n",
            "right to herself, 'I suppose I growl who was the use and the song. and the first was opened \n",
            "'No, I give the children said Alice; 'only, as she found she thought, and was just \n",
            "\n",
            "repeated thoughtfully. by everybody \n",
            "key in with \n",
            "\n",
            "'What do wish it say, \n",
            "key in a little recovered its voice. \n",
            "\n",
            "\n",
            "'So you see, when it was nothing had a growl, so she had no pleasing the wind, mushroom but they walked arm her saucer ran off into \n",
            "now? her next witness!' said Alice was nothing written the top \n",
            "ever saw is the trial's their \n",
            "'You may SIT \n",
            "And she's a bough you may be sure! \n",
            "her saucer of a little ledge up and the door which very good-naturedly the March Hare will burn \n",
            "\n",
            "\n",
            "Soon that had just timidly, the March Hare she had just at Alice. \n",
            "'It of execution. \n",
            "\n",
            "'No, 'I suppose it \n",
            "'What a growl, \n",
            "'Boots roses. \n",
            "\n",
            "jumping up towards you see.' \n",
            "Alice (she \n",
            "'Boots and was \n",
            "\n",
            "though \n",
            "his confusion \n",
            "Luckily said Alice a growl, she heard of his history. you a little bat! \n",
            "'Oh, \n",
            "which were learning \n",
            "\n",
            "to begin lessons: said the Queen had no pleasing she was still to go round the use in silence. \n",
            "'I'd you see.' \n",
            "which was nothing had no pleasing and got so she could remember them, they walked off into the use and Alice (she \n",
            "his father; knock, and Alice was still in her usual the confused \n",
            "'Boots and Alice a growl, she felt very good-naturedly a growl, \n",
            "growing, footman because very good-naturedly a wink look at Alice. \n",
            "First, she had a \n",
            "However, \n",
            "\"What AT a wink himself in a bough you know I shan't stand on its voice. \n",
            "This piece with his history. kick a wink did not a table, \n",
            "through the pig-baby said Alice (she \n",
            "key was still a snail. never been of a wink hurt the pig-baby \n",
            "'You can very grave it as she got so she had no pleasing of the March Hare will you, will you, than his history. \n",
            "\n",
            "'Yes, it as the time this bottle nothing had just at Alice. \n",
            "\n",
            "with his confusion is all day to go round and Alice was still even if it would not help stood looking at school to begin lessons: said Alice. \n",
            "key \n",
            "yet.' \n",
            "ever see was just at Alice. \n",
            "\"What and was still and, as the moment Alice in the children said the words is the trial's skurried whether \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5DAg0nj0vTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for epoch in range(num_epochs):\n",
        "#   # Set initial hidden and cell states\n",
        "#   states = (torch.zeros(num_layers, batch_size, hidden_size),\n",
        "#             torch.zeros(num_layers, batch_size, hidden_size))\n",
        "  \n",
        "#   for i in range(0, rep_tensor.size(1)-timesteps, timesteps):\n",
        "#     # Get mini-batch inputs and targets\n",
        "#     inputs = rep_tensor[:, i:i+timesteps]   # --> (:, 0:0+30), output-> (:, 1+31)\n",
        "#     targets = rep_tensor[:, (i+1):(i+1)+timesteps]\n",
        "#     # String : Black Horse is here\n",
        "#     # input: Black Horse    Output: lack Hourse i\n",
        "\n",
        "#     outputs, _ = model(inputs, states)\n",
        "#     loss = loss_fn(outputs, targets.reshape(-1))\n",
        "\n",
        "#     # Backpropagation and Wegith Update\n",
        "#     model.zero_grad()\n",
        "#     loss.backward()\n",
        "#     # Perform Gradient Clipping. clip_value (float or int) is the maximum allowed value of gradients\n",
        "#     # The gradients are clipped in the range [-clip_value, cli_value]. This is to prevent the exploding gradient problem\n",
        "#     clip_grad_norm(model.parameters(), 0.5)\n",
        "#     optimizer.step()\n",
        "\n",
        "\n",
        "#     step = (i+1) // timesteps\n",
        "#     if step%100 == 0:\n",
        "#       print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}